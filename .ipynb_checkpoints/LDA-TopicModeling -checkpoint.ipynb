{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os, sys, glob\n",
    "import re\n",
    "from ast import literal_eval\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from collections import Counter\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data from google cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#read files\n",
    "def read_gs(path, id_name):\n",
    "    dev_file = tf.gfile.Glob(path)[0]\n",
    "    df = pd.read_csv(tf.gfile.Open(dev_file, mode='rb'), \n",
    "                     escapechar='\\\\', \n",
    "                     error_bad_lines=False, \n",
    "                     header=0, \n",
    "                     dtype={'original_id': str})\n",
    "    df.rename(columns={'original_id': id_name}, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "path = 'gs://directory/'\n",
    "shop_dir = path + '/*.csv'\n",
    "shopDf = read_gs(shop_dir, \"shop_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace column with another column according to condition given\n",
    "shopDf = shopDf[shopDf['tags_th'] != \"['null']\"]\n",
    "shopDf['tags_th'] = np.where(shopDf['tags_th'] == '[]', shopDf['tags_en'], shopDf['tags_th'])\n",
    "shopDf['title_th'] = np.where(shopDf['title_th'].isnull(), shopDf['title_en'], shopDf['title_th'])\n",
    "\n",
    "tags_en = shopDf['tags_en']\n",
    "tags_th = shopDf['tags_th']\n",
    "\n",
    "def literal(i):\n",
    "    return literal_eval(i.replace(\"['\",\"[\\\"\").replace(\"']\", \"\\\"]\").replace(\"','\", \"\\\",\\\"\"))\n",
    "\n",
    "tags = tags_th.map(literal)\n",
    "print (len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def doc_lens(doc):\n",
    "    \n",
    "    document_lengths = np.array(list(map(len, doc)))\n",
    "    print(\"The average number of words in a document is: {}.\".format(np.mean(document_lengths)))\n",
    "    print(\"The minimum number of words in a document is: {}.\".format(min(document_lengths)))\n",
    "    print(\"The maximum number of words in a document is: {}.\".format(max(document_lengths)))\n",
    "    \n",
    "    return document_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "document_lengths = doc_lens(tags)\n",
    "shorten_length = 30\n",
    "print(\"There are {} documents with over {} words.\".format(sum(document_lengths > shorten_length), shorten_length))\n",
    "shorter_documents = document_lengths[document_lengths <= shorten_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove word that ends with s\n",
    "#### Don't want to apply stemming and lematization because it effects shop's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_word(documents):\n",
    "#     pattern_s = \"([^e+u+s\\s])s$\"\n",
    "    pattern_s = \"([^s\\s])s$\"\n",
    "    for i in documents:\n",
    "        for j, item in enumerate(i):\n",
    "            item = item.lower()\n",
    "            if re.search(re.compile(pattern_s), item) != None:\n",
    "                item = item[:-1]\n",
    "            i[j] = item\n",
    "\n",
    "    return documents\n",
    "\n",
    "tags = filter_word(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the text using nltk's word tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "documents = []\n",
    "for tag in tags:\n",
    "    g = []\n",
    "    for word in tag:\n",
    "        #Tokenize a string to split off punctuation other than periods\n",
    "        words = nltk.word_tokenize(word)\n",
    "        for current_word in words:\n",
    "            current_word = current_word.lower()\n",
    "            g.append(current_word)\n",
    "    documents.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_lengths = doc_lens(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "ax.set_title(\"Distribution of number of words\", fontsize=16)\n",
    "ax.set_xlabel(\"Number of words\")\n",
    "sns.distplot(document_lengths, bins=50, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_len = 30\n",
    "print(\"There are {} documents with over {} words.\".format(sum(document_lengths > exclude_len), exclude_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "ax.set_title(\"Distribution of number of words\", fontsize=16)\n",
    "ax.set_xlabel(\"Number of words\")\n",
    "sns.distplot(shorter_documents);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show two-word document\n",
    "The shorter documents will probably be harder to classify since we'll have less words to cling to. LDA for example tries to find topics in documents, but if the documents are so short, perhaps it will find it hard to really find a topic in a two-word document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in documents if len(i) <= 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot total number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allword = flatten(documents)\n",
    "print (len(allword))\n",
    "print (len(set(allword)))\n",
    "word = 300\n",
    "most_common = Counter(allword).most_common(word)\n",
    "least_common = Counter(allword).most_common()[-word-1:-1]\n",
    "x = [i[0] for i in most_common]\n",
    "y = [i[1] for i in most_common]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "ax.set_title(\"Distribution of word frequency\", fontsize=16)\n",
    "ax.set_xlabel(\"word\")\n",
    "sns.barplot(x,y)\n",
    "# most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "most_common[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From above chart, we choose more than 3 frequency of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "for text in documents:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "documents1 = [[token for token in text if frequency[token] >= 3 ] for text in documents]\n",
    "flat_doc1 = flatten(documents1)\n",
    "print ('Lenghts of total word {}'.format(len(flat_doc1)))\n",
    "print ('Lengths unique word {}'.format(len(set(flat_doc1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove word less than 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "documents_cha = []\n",
    "for tag in documents:\n",
    "    g = []\n",
    "    for word in tag:\n",
    "        if len(word) >= 3 or word == \"ยา\" or word == \"ชา\":\n",
    "            g.append(word)\n",
    "    documents_cha.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "for text in documents_cha:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "documents_freq = [[token for token in text if frequency[token] >= 3] for text in documents_cha]\n",
    "\n",
    "flat_doc1 = flatten(documents_freq)\n",
    "print ('Lenghts of total word {}'.format(len(flat_doc1)))\n",
    "print ('Lengths unique word {}'.format(len(set(flat_doc1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unwanted word those are Thailand province, district, and road in both Thai and English that are contained in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "province = np.load('province.npy')\n",
    "print (len(province))\n",
    "district = np.load('district.npy')\n",
    "road = np.load('road.npy')\n",
    "exclude_word = list(province) + list(district) + list(road)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dictionary and Corpus needed for LDA -Topic Modeling\n",
    "\n",
    "Dictionary is a unique id for each word in the document.\n",
    "\n",
    "Corpus is a mapping of (word_id, word_frequency).\n",
    "\n",
    "Remove exclude from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dictionary\n",
    "dictionary = Dictionary.from_documents(documents_freq)\n",
    "# ignore words that appear in less than 3 documents or more than 80% documents\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.9)\n",
    "dictionary.compactify()\n",
    "# remove exclude word from dictionary \n",
    "del_ids = [k for k,v in dictionary.items() if v in exclude_word]\n",
    "dictionary.filter_tokens(bad_ids=del_ids)\n",
    "dictionary.compactify()\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the cleansed words frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleansed_words_df = pd.DataFrame.from_dict(dictionary.token2id, orient='index')\n",
    "cleansed_words_df.rename(columns={0: 'id'}, inplace=True)\n",
    "cleansed_words_df['count'] = list(map(lambda id_: dictionary.dfs.get(id_), cleansed_words_df.id))\n",
    "cleansed_words_df.drop(['id'], axis=1, inplace=True)\n",
    "cleansed_words_df.sort_values('count', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency_barplot(df, nr_top_words=50):\n",
    "    \"\"\" df should have a column named count.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,1,figsize=(20,5))\n",
    "    sns.barplot(list(range(nr_top_words)), df['count'].values[:nr_top_words], palette='hls', ax=ax)\n",
    "    \n",
    "    ax.set_xticks(list(range(nr_top_words)))\n",
    "    ax.set_xticklabels(df.index[:nr_top_words], fontsize=14, rotation=90)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cleansed_words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ax = word_frequency_barplot(cleansed_words_df)\n",
    "ax.set_title(\"Document Frequencies (Number of documents a word appears in)\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf score\n",
    "Tf-idf reflects how important a word is to a document in a collection or corpus.\n",
    "\n",
    "The higher the Tf-idf score (weight), the rarer the term and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_model = gensim.models.TfidfModel(corpus, id2word=dictionary)\n",
    "low_value = 0.1\n",
    "low_value_words = []\n",
    "for bow in corpus:\n",
    "    low_value_words += [id for id, value in tfidf_model[bow] if value < low_value]\n",
    "\n",
    "#filter value tfidf > 0.9 (unique words that we don't want to feed in LDA)\n",
    "dictionary.filter_tokens(bad_ids=low_value_words)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LDA Model\n",
    "In addition to the corpus and dictionary, you need to provide the number of topics as well.\n",
    "LDA model is built with 5 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.\n",
    "\n",
    "The weights reflect how important a keyword is to that topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the optimal number of topics for LDA\n",
    "Build many LDA models with different values of number of topics and pick the one that gives the highest coherence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model= gensim.models.ldamodel.LdaModel(corpus=corpus, \n",
    "                                           num_topics=num_topics,  \n",
    "                                           id2word=dictionary,\n",
    "                                            )\n",
    "        \n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot coherence score to choose the best number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start=2\n",
    "limit=20\n",
    "step=3\n",
    "corpus = corpus\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, \n",
    "                                                        texts=documents, start=start, limit=limit, step=step)\n",
    "# Print the coherence scores\n",
    "print ('Plotting graph')\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n",
    "\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    round_cv = round(cv, 4)\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose number of topic that have the best coherence score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "optimal_model = gensim.models.ldamodel.LdaModel.load(\"lda.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_list[2]\n",
    "#save model\n",
    "model.save('lda.model')\n",
    "model_topics = model.show_topics(formatted=True)\n",
    "print (len(model_topics))\n",
    "model_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Model Perplexity and Coherance Score\n",
    "The higher coherance Score, the better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=model, texts=documents, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the topics keywords\n",
    "A good topic model should have non-overlapping cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(model, corpus, dictionary)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look into LDA result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopDf['group'] = [max(model[i], key=lambda x:x[1])[0] for i in corpus]\n",
    "shopDf['prob'] = [max(model[i], key=lambda x:x[1])[1] for i in corpus]\n",
    "shopDf['tags_other'] = [max(model[i], key=lambda x:x[1])[0] if (max(model[i], key=lambda x:x[1])[1] > 0.6) else 99 for i in corpus ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(tag_group, column, shopDf):\n",
    "    \n",
    "    result = shopDf[shopDf[column] == tag_group][['merchant_id', 'title_th', 'tags_th']]\n",
    "    print ('Number of result is...\\n{}'.format(result.count()))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = result(0, 'tags_other', shopDf)\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting top threshold of probability of each word assigned to a topic or select top n word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_exact_match(data):\n",
    "\n",
    "    for cate in SHOP.keys():\n",
    "        for val in SHOP[cate]:\n",
    "            if re.search(r'\\b{0}\\b'.format(val.lower()), data.lower()):\n",
    "                return cate\n",
    "    return 99\n",
    "\n",
    "def title_some_match(data):\n",
    "\n",
    "    for cate in SHOP.keys():\n",
    "        for val in SHOP[cate]:\n",
    "            if val.lower() in data.lower():\n",
    "                return cate\n",
    "    return 99\n",
    "\n",
    "def percent_trash():\n",
    "    shopDf['group'] = shopDf['tags_th'].map(title_exact_match)\n",
    "    #replace bin 99 with group that have matched word\n",
    "    shopDf['group'] = np.where(shopDf['group'] == 99, shopDf['tags_th'].map(title_some_match), shopDf['group'])\n",
    "    extra_bin = shopDf[shopDf['group'] == 99]['merchant_id'].count()\n",
    "    prop_trash = extra_bin / total_doc \n",
    "    \n",
    "    return prop_trash\n",
    "\n",
    "def plot_word_trash(word_range, trash):\n",
    "    plt.plot(word_range, trash, color='b')\n",
    "    plt.xlabel('Top n')\n",
    "    plt.ylabel('Proportion of trash to total number of document')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top n word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trash = []\n",
    "total_doc = shopDf['merchant_id'].count()\n",
    "range_word = range(1, 10)\n",
    "for topword in range_word:\n",
    "    keyword = []\n",
    "    for t in range(len(model_topics)):\n",
    "        wordlist = (model.show_topic(t, topword))\n",
    "        keyword.append([x for x,_ in wordlist])\n",
    "    SHOP = dict(enumerate(keyword, start=1))\n",
    "    print ('\\nTop {} word...'.format(topword))\n",
    "    print (SHOP)\n",
    "\n",
    "    prop_trash = percent_trash()\n",
    "    trash.append(prop_trash)\n",
    "\n",
    "plot_word_trash(range_word, trash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top n threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = []\n",
    "trash = []\n",
    "threshold = [0.04, 0.03, 0.02]\n",
    "for i in threshold:\n",
    "    for t in range(len(model_topics)):\n",
    "        wordlist = (model.show_topic(t, topn=20))\n",
    "        keyword.append([x for x, y in wordlist if y > i])\n",
    "    SHOP = dict(enumerate(keyword, start=1))\n",
    "    print ('\\nTop {} threshold...'.format(i))\n",
    "    print (SHOP)\n",
    "\n",
    "    prop_trash = percent_trash()\n",
    "    trash.append(prop_trash)\n",
    "plot_word_trash(threshold, trash)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
